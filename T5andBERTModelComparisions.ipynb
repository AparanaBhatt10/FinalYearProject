{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "T5andBERTModelComparisions.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_43JlTh2feL",
        "outputId": "1bdd737c-a910-4773-bb4e-167bf47f1cd8"
      },
      "source": [
        " !pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 27.7 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 4.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CAn5GTo2h_b",
        "outputId": "eb9f8b67-a760-496c-b628-b668b4fb5a29"
      },
      "source": [
        "!pip install transformers -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.6 MB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 48.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 39.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 46.4 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKCAPCnFy9jy",
        "outputId": "86d41a99-c2f7-480b-a074-722e69726c63"
      },
      "source": [
        "!curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  5116  100  5116    0     0  12854      0 --:--:-- --:--:-- --:--:-- 12854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw8TBnIdzDHo",
        "outputId": "027e2234-8cba-423f-a330-b2a6ebf0297b"
      },
      "source": [
        "!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updating... This may take around 2 minutes.\n",
            "Found existing installation: torch 1.9.0+cu102\n",
            "Uninstalling torch-1.9.0+cu102:\n",
            "  Successfully uninstalled torch-1.9.0+cu102\n",
            "Found existing installation: torchvision 0.10.0+cu102\n",
            "Uninstalling torchvision-0.10.0+cu102:\n",
            "  Successfully uninstalled torchvision-0.10.0+cu102\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly+20200515-cp37-cp37m-linux_x86_64.whl...\n",
            "\\ [1 files][ 91.0 MiB/ 91.0 MiB]                                                \n",
            "Operation completed over 1 objects/91.0 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20200515-cp37-cp37m-linux_x86_64.whl...\n",
            "| [1 files][119.5 MiB/119.5 MiB]                                                \n",
            "Operation completed over 1 objects/119.5 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20200515-cp37-cp37m-linux_x86_64.whl...\n",
            "- [1 files][  2.3 MiB/  2.3 MiB]                                                \n",
            "Operation completed over 1 objects/2.3 MiB.                                      \n",
            "Processing ./torch-nightly+20200515-cp37-cp37m-linux_x86_64.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==nightly+20200515) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==nightly+20200515) (1.19.5)\n",
            "Installing collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 1.0.61 requires torchvision, which is not installed.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.6.0a0+bf2bbd9 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.6.0a0+bf2bbd9\n",
            "Processing ./torch_xla-nightly+20200515-cp37-cp37m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+2b2085a\n",
            "Processing ./torchvision-nightly+20200515-cp37-cp37m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchvision==nightly+20200515) (1.6.0a0+bf2bbd9)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==nightly+20200515) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==nightly+20200515) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch->torchvision==nightly+20200515) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.7.0a0+a6073f0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libopenblas-dev is already the newest version (0.2.20+ds-4).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 2s (145 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 148486 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "z4eyTGOR3RHJ",
        "outputId": "bc16c807-1ab7-4509-a24f-b2c54c0896ee"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.6.0\n"
          ]
        },
        {
          "ename": "BaseException",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d8fa1f5d0446>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mtpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUClusterResolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TPU detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running on TPU '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'worker'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tpu, zone, project, job_name, coordinator_name, coordinator_address, credentials, service, discovery_url)\u001b[0m\n\u001b[1;32m    206\u001b[0m           \u001b[0mservice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m           discovery_url=discovery_url)\n\u001b[0m\u001b[1;32m    208\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cloud_tpu_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/tpu/client/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tpu, zone, project, credentials, service, discovery_url)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtpu\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Please provide a TPU Name to connect to.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Please provide a TPU Name to connect to.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mBaseException\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d8fa1f5d0446>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running on TPU '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'worker'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_connect_to_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBaseException\u001b[0m: ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGbJz3I2zOL-",
        "outputId": "4861202b-ba8c-4e11-cfbf-7fa4a6f7ce32"
      },
      "source": [
        "!pip install pytorch_lightning==0.7.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch_lightning==0.7.5\n",
            "  Downloading pytorch_lightning-0.7.5-py3-none-any.whl (233 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 26.8 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 71 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 81 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 92 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 112 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 122 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 133 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 143 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 153 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 163 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 174 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 184 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 194 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 204 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 215 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 225 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 233 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==0.7.5) (2.6.0)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==0.7.5) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==0.7.5) (1.6.0a0+bf2bbd9)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 45.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==0.7.5) (4.62.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (2.23.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.39.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (0.12.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (0.4.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (0.6.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (3.17.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.34.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (0.37.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (3.3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=1.14->pytorch_lightning==0.7.5) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.7.5) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.7.5) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.7.5) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.7.5) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.7.5) (3.7.4.3)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=ee9af18a51e92fee90267141b6ec2fe29d55401a14919f15b0a9cc909ecdb87e\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: future, pytorch-lightning\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed future-0.18.2 pytorch-lightning-0.7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8_4pFalIX95",
        "outputId": "ad0ddcb0-9478-4361-fb0f-6137dd386e47"
      },
      "source": [
        "import transformers \n",
        "print(transformers.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqUWY6lUA5ET",
        "outputId": "aad70182-ea5a-4b14-999d-bd978291608b"
      },
      "source": [
        "import torch \n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.6.0a0+bf2bbd9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4nGOf2gytei"
      },
      "source": [
        "# Importing stock libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEhNgaHEy0rg",
        "outputId": "9cbc4d5a-7106-4c8d-e6cd-d18e34978183"
      },
      "source": [
        "# Checking out the GPU we have access to. This is output is from the google colab version. \n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Aug 23 16:42:49 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y836TxZ1DIp",
        "outputId": "1e045ad9-eb29-4c3c-c713-c603a5f895b4"
      },
      "source": [
        "# # Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "print(str(device))\n",
        "\n",
        "# Preparing for TPU usage\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "# device = xm.xla_device()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vsWo7qw1G2G"
      },
      "source": [
        "# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "        self.text = self.data.text\n",
        "        self.ctext = self.data.ctext\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ctext = str(self.ctext[index])\n",
        "        ctext = ' '.join(ctext.split())\n",
        "\n",
        "        text = str(self.text[index])\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt',truncation=True)\n",
        "        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt',truncation=True)\n",
        "\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long), \n",
        "            'source_mask': source_mask.to(dtype=torch.long), \n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V43twlfn601N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3r1XFV260yC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_RxuXny1bv3"
      },
      "source": [
        "# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n",
        "# The model is put into train mode and then we wnumerate over the training loader and passed to the defined network \n",
        "\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    model.train()\n",
        "    for _,data in enumerate(loader, 0):\n",
        "        y = data['target_ids'].to(device, dtype = torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
        "        loss = outputs[0]\n",
        "        \n",
        "        if _%500==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "            #print(str(outputs))\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # xm.optimizer_step(optimizer)\n",
        "        # xm.mark_step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1sxlL1d1k15"
      },
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data['target_ids'].to(device, dtype = torch.long)\n",
        "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask, \n",
        "                max_length=200, \n",
        "                num_beams=2,\n",
        "                repetition_penalty=3.5, \n",
        "                length_penalty=1.0, \n",
        "                early_stopping=True\n",
        "                )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "            if _%100==0:\n",
        "                print(f'Completed {_}')\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "    return predictions, actuals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaMZn5m32Vcj",
        "outputId": "e0713e5c-4ede-4619-bd4a-0016dcd0dfa9"
      },
      "source": [
        "!pip install rouge-score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.19.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.2.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.12.0)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vue_IF7YtNlg",
        "outputId": "f787bcd9-4f95-4374-982c-b0e6b5b0bbb8"
      },
      "source": [
        "import pandas as pd\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "  \n",
        "  \n",
        "df = pd.read_csv('/content/SummaryDataSetCustom.tsv',encoding='latin-1',delimiter=\"\\t\")\n",
        "df = df[['STORY','SUMMARY']]\n",
        "df.STORY = 'summarize: ' + df.STORY\n",
        "df['text']=df['STORY']\n",
        "df['ctext']=df['SUMMARY']\n",
        "print(df.head())\n",
        "df=df.head(100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               STORY  ...                                              ctext\n",
            "0  summarize: Business Wire IndiaCSS Corp, a glob...  ...  CSS Corp, a global customer experience and tec...\n",
            "1  summarize: Business Wire IndiaSmiths Detection...  ...                                                NaN\n",
            "2  summarize: Business Wire India ECOVACS is pres...  ...  ECOVACS ROBOTICS, a market leader in innovativ...\n",
            "3  summarize: Business Wire India  New analyses f...  ...  New analyses from pivotal BAVENCIOÂ® study rei...\n",
            "4  summarize: Business Wire IndiaTakeda Pharmaceu...  ...            Takeda Pharmaceutical Company Limited (\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsjmXMfgr-f8",
        "outputId": "d2741589-7c31-4ebd-bf91-31d779bdda82"
      },
      "source": [
        "import pandas as pd\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "  \n",
        "  \n",
        "df = pd.read_csv('/content/summary-dataset.csv',encoding='latin-1',delimiter=\"$\")\n",
        "df = df[['NEWS','SUMMARY']]\n",
        "df.NEWS = 'summarize: ' + df.NEWS\n",
        "df['text']=df['NEWS']\n",
        "df['ctext']=df['SUMMARY']\n",
        "print(df.head())\n",
        "df=df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                NEWS  ...                                              ctext\n",
            "0  summarize: Job cuts 'false economy'  - TUC  Pl...  ...  Plans to shed 71,000 civil service jobs will p...\n",
            "1  summarize: Probe launched on Ken Nazi jibe  An...  ...  Ms Gavron said she thought Mr Livingstone's co...\n",
            "2  summarize: Blair blasts Tory spending plans  T...  ...  In his speech, Mr Blair contrasted a reformed ...\n",
            "3  summarize: Iraq advice claim sparks new row  T...  ...  \"After that Downing Street proceeded to set ou...\n",
            "4  summarize: Women MPs reveal sexist taunts  Wom...  ...  But she said there was a difference between th...\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQM3Rlc2aAT2"
      },
      "source": [
        "from rouge_score import rouge_scorer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkmFmYYV12JX"
      },
      "source": [
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    TRAIN_BATCH_SIZE = 5    # input batch size for training (default: 64)\n",
        "    VALID_BATCH_SIZE = 5    # input batch size for testing (default: 1000)\n",
        "    TRAIN_EPOCHS = 2   # number of epochs to train (default: 10)\n",
        "    VAL_EPOCHS = 1 \n",
        "    LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
        "    SEED = 42               # random seed (default: 42)\n",
        "    MAX_LEN = 512\n",
        "    SUMMARY_LEN = 150 \n",
        "\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "    torch.manual_seed(SEED) # pytorch random seed\n",
        "    np.random.seed(SEED) # numpy random seed\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # tokenzier for encoding the text\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "    #print(str(tokenizer))\n",
        "    \n",
        "\n",
        "    # Importing and Pre-Processing the domain data\n",
        "    # Selecting the needed columns only. \n",
        "    # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n",
        "  \n",
        "\n",
        "    \n",
        "    # Creation of Dataset and Dataloader\n",
        "    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n",
        "    train_size = 0.8\n",
        "    train_dataset=df.sample(frac=train_size, random_state = SEED).reset_index(drop=True)\n",
        "    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "\n",
        "    print(\"FULL Dataset: {}\".format(df.shape))\n",
        "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
        "\n",
        "\n",
        "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
        "    training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
        "    val_set = CustomDataset(val_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
        "\n",
        "\n",
        "    # Defining the parameters for creation of dataloaders\n",
        "    train_params = {\n",
        "        'batch_size': TRAIN_BATCH_SIZE,\n",
        "        'shuffle': True,\n",
        "        'num_workers': 0\n",
        "        }\n",
        "\n",
        "    val_params = {\n",
        "        'batch_size': VALID_BATCH_SIZE,\n",
        "        'shuffle': False,\n",
        "        'num_workers': 0\n",
        "        }\n",
        "\n",
        "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "\n",
        "    \n",
        "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
        "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "  #  model = pd.read_pickle(r'/content/custom-T5')\n",
        "    model = model.to(\"cpu\")\n",
        "\n",
        "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
        "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # Training loop\n",
        "    print('Initiating Re-Training for the model on our dataset')\n",
        "    \n",
        "    for epoch in range(TRAIN_EPOCHS):\n",
        "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "    return epoch,tokenizer,model,device,val_loader\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "YwSuqBjYFi72",
        "outputId": "6719d4d1-ed84-4701-f917-833b8908d982"
      },
      "source": [
        "epoch,tokenizer,model,device,val_loader=main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FULL Dataset: (100, 4)\n",
            "TRAIN Dataset: (80, 4)\n",
            "TEST Dataset: (20, 4)\n",
            "Initiating Re-Training for the model on our dataset\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-334ce45e46a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-71b298a615ec>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBa-yj5WaThl"
      },
      "source": [
        "def getSummarisedText(data):\n",
        "    text=data\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "    # Generating summary ids\n",
        "    summary_ids = bert_from_pickle.generate(\n",
        "        input_ids=input_ids,\n",
        "\n",
        "        max_length=150,\n",
        "        num_beams=2,\n",
        "        repetition_penalty=3.5,\n",
        "        length_penalty=1.0,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    # Decoding the tensor and printing the summary.\n",
        "    t5_summary = tokenizer.decode(summary_ids[0])\n",
        "    return t5_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKmjXoWZbVfl",
        "outputId": "68326566-be57-4c1f-b264-e3f7f7b5904c"
      },
      "source": [
        "actuals= df[\"SUMMARY\"].tolist()\n",
        "print(str(actuals))\n",
        "print(len(actuals))\n",
        "predicted=[]\n",
        "maintext=df['NEWS'].tolist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Plans to shed 71,000 civil service jobs will prove to be a \"false economy\" that could hamper public sector reforms, according to a TUC report.\"Public services are improving but looking for simple savings through job cuts at this stage could be a false economy.\"They may shoot a Tory fox, but cutting thousands of civil service jobs will hit the morale and capabilities of the public servants expected to implement government reforms.Public and Commercial Services union members have already voted to strike over cuts for one day on 5 November.General secretary Brendan Barber warned the \"costs could easily outweigh the benefits\".The costs could easily outweigh the benefits.\"', 'Ms Gavron said she thought Mr Livingstone\\'s comments were \"inappropriate\" but she did not believe the mayor was anti-Semitic.Speaking after the investigation was announced Bob Neill, leader of the London Assembly Conservatives, said: \"He has behaved in a manner unbecoming of his office and in so doing, has shown extraordinarily poor civic leadership.An investigation by the Standards Board is under way following allegations that Ken Livingstone has brought his office into disrepute.On Sunday, Deputy Mayor Nicky Gavron told the BBC\\'s Politics Show she believed the Mayor of London would say sorry on Tuesday for offending the wider Jewish community.It has the power to suspend or bar Labour\\'s Mr Livingstone from office.The Prime Minister Tony Blair is among those who have called for an apology but so far the mayor has refused.', 'In his speech, Mr Blair contrasted a reformed Labour party, which had learned to occupy the political centre ground, with a hidebound Tory party, which he said would turn the clock back with spending cuts.Tory leader Michael Howard has said his party would cut Â£35bn in \"wasteful\" spending to allow Â£4bn in tax cuts.Mr Blair said: \"The Conservative tax and spending proposals would put at risk, both Britain\\'s hard-won economic stability - the lowest mortgages, inflation, unemployment, for decades - and the key investment in public services.The Chancellor said the Conservatives\\' plans would see some Â£50bn in spending cuts by 2011, which the Tories deny.Meanwhile, Tory shadow chancellor Oliver Letwin accused Mr Blair of \"misrepresenting\" the party\\'s proposals and questioned how Labour would fund its own plans.On Saturday, Tory shadow home secretary David Davis said the Tories would fund the cuts by removing \"inefficiencies\" which had \"burgeoned\" under Labour.The prime minister has told a Labour Party gathering that the Tory policies would cause economic failure.In response, David Davis said the Tories would make cuts, such as removing regional assemblies, but would bring in more police officers and match Labour\\'s spending on health and education.Mr Davis said Labour had been responsible for \"huge waste, huge overspending, not on the frontline at all but on bureaucracy\".\"Once again Mr Blair and his Chancellor have failed to answer the question that lies at the heart of this election - which taxes will they put up to fill the Â£8bn shortfall in their plans?\"', '\"After that Downing Street proceeded to set out his [Lord Goldsmith\\'s] view in a parliamentary answer which was then published on 17 March,\" said Mr Sands.Downing Street has denied the claims, made in a new book about the Attorney General Lord Goldsmith\\'s advice.But a short statement about Lord Goldsmith\\'s position was presented in a written parliamentary answer on 17 March 2003 - just before a crucial Commons vote on the military action.In a statement to Newsnight, Lord Goldsmith said: \"In my parliamentary answer on March 17 2003, I explained my genuinely held independent view, that military action was lawful under the existing Security Council resolutions.This would be wrong even if Lord Goldsmith had signed the statement, Mr Cook said, because the attorney general\\'s advice should be an \"independent legal opinion\", not subject to \"political negotiation of this kind\".Lord Goldsmith also denied them, saying he was not \"leaned on\" in any way.The government has consistently refused to publish Lord Goldsmith\\'s advice on the legality of the war - saying such papers have always been kept confidential.The Tories say ministers must respond in Parliament to claims that the legal advice used to justify the Iraq war was drawn up at Number 10.Liberal Democrat foreign affairs spokesman Menzies Campbell repeated his party\\'s calls for Lord Goldsmith\\'s first piece of legal advice to be made public.But the Conservatives and Liberal Democrats say they want the publication of the full legal advice given by the Attorney General.', 'But she said there was a difference between the experiences of women before the 1997 intake and afterwards.Even after the great influx of women MPs at the 1997 general election, and greater numbers of women in the Cabinet, female MPs often say they feel stuck on the edge of a male world.This was mainly because there were more women present in Parliament who were not prepared to \"put up with\" the sexist attitudes they came across, Prof Lovenduski said.Male MPs pretended to juggle imaginary breasts and jeered \"melons\" as women made Commons speeches, researchers from Birkbeck College were told.Prof Joni Lovenduski, who conducted the study with the help of Margaret Moran MP and a team of journalists, said she was shocked at the findings.But she added: \"Some women, including the women who came in 1997, received extraordinary treatment and I am not convinced that if the number of women changed back to what it was before 1997 that things would not change back.Labour\\'s Yvette Cooper said she found it hard to persuade Commons officials she was a minister and not a secretary.Women MPs endure \"shocking\" levels of sexist abuse at the hands of their male counterparts, a new study shows.And ex-Tory education secretary Gillian Shepherd remembered how one of her male colleagues called all women \"Betty\".Barbara Follet, one of the so-called \"Blair Babes\" elected in 1997, told researchers: \"I remember some Conservatives - whenever a Labour woman got up to speak they would take their breasts - imaginary breasts - in their hands and wiggle them and say \\'melons\\' as we spoke.\"', \"Registered political parties are required to set out each quarter all donations over Â£5,000 to their headquarters and over Â£1,000 to local constituency parties they receive.The Labour Party received more than Â£5m in donations in the final quarter of 2004, new figures show.Other significant donations came from retired millionaire businessman and philanthropist Sir Christopher Ondaatje who gave the party a sum of Â£500,000, and refrigerator magnate William Haughey OBE who gave Â£330,000.The largest donation to the Conservatives was a bequest from Ruth Beardmore of nearly Â£400,000.The Conservatives were in second place with donations totalling Â£4,610,849, while the Liberal Democrats received just over Â£1m.The Liberal Democrats' largest donor was the Joseph Rowntree Reform Trust Ltd, a company which promotes political reform and constitutional change, which gave a sum of Â£250,000.This is nearly half of the Â£11,724,929 received by 16 political parties listed by the Electoral Commission.There were also donations topping Â£250,000 for the Conservatives from Scottish Business Groups Focus on Scotland and the Institute of International Research, the world's largest independent conference company.Also among the gifts to the Tories were 24 donations totalling Â£161,840 from Bearwood Corporate Services.\", 'Ms Jamieson said the executive should have a role in tackling the soccer troublemakers.Ms Jamieson praised the police for their action and said: \"The police do want to identify whether there are particular individuals who are going over the top and inciting hatred or violence - they will crack down very effectively on them.Ms Jamieson stressed that sectarianism has not been confined to football but it can act as a \"trigger\" for tensions and violence.Cathy Jamieson said exclusion orders are one of a series of measures being considered in the Scottish Executive campaign against sectarianism.\"That\\'s the kind of thing that will hit those kind of people where it hurts the most in not allowing them to attend the games,\" she said.Speaking on BBC Radio Scotland\\'s Sunday Live programme, Ms Jamieson described Friday\\'s meeting as \"very productive\" and said putting the squeeze on the bigots would be a key aim.\"Both Rangers and Celtic football clubs have been involved in working with the executive to produce, for example, an educational pack for young people.\"Last week Ms Jamieson and First Minister Jack McConnell met supporters\\' representatives from both clubs to discuss the issue.', 'He said: \"You\\'ve got to bring back school sport.The prime minister launched a Â£500m initiative to allow school sports clubs in England to provide up to three hours of extra activity a week by 2010.Shadow spokesman for sport Hugh Robertson said: \"I suspect the correct way to tackle it is to look at the other end of the spectrum and try to enable the clubs - which is where the real passion for sport exists - to deliver the school sport offer.\"But the Conservatives warned of rising obesity levels and said that since only a third of children do two hours of sport a week currently, the government is promising something it cannot guarantee.\"It\\'s an important part of education and it\\'s an important part of health,\" Mr Blair said.The Â£500m move will also increase the number of specialist sport colleges from 350 to 400.He said more emphasis should be put on traditional sports, saying that an \"anti-competitive sport agenda\" had been pursued in recent years.Tony Blair has promised that \"sport is back\" as a priority for schools.', \"In July 1999 they asked 801 adults if they supported the ban for the Mail on Sunday.The Mail on Sunday survey found that 63% supported a hunting ban compared with 24% against.Less than half the UK wants a ban compared to almost two-thirds in 1999, the Mori survey of 2,000 adults for BBC One's Countryfile programme suggests.Support for a ban on hunting has fallen in the past six years, a poll suggests.\", 'And the law protects those who use \"something to hand\" as a weapon, said the leaflet published jointly by the Crown Prosecution Service (CPS) and Association of Chief Police Officers (ACPO).In cases involving householders attacking intruders prosecutors and police were \"determined\" they would be dealt with \"as swiftly and as sympathetically as possible\", it said.The Tories have called for a change in the law so householders are only prosecuted if they use \"grossly disproportionate\" force.Doing what you \"honestly and instinctively\" believed was necessary would be the strongest evidence of acting lawfully, the guidance said.The guidance published on Tuesday, said the police had a duty to investigate all incidents involving a death or injury.A CPS spokesperson said the figures were not definitive because prosecutions are not listed according to whether they were committed by a householder on an intruder.The leaflet, published by police and prosecutors, aims to combat confusion about current legislation, which lets people use \"reasonable force\".The law also protects those who use \"something to hand\" as a weapon.But knocking someone unconscious then killing them or hurting them further, or setting a trap for an intruder without involving the police were given as examples of \"excessive and gratuitous\" force.']\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdLNhfryaxk4",
        "outputId": "6ec4d460-6914-4cea-9a91-a40d1783b854"
      },
      "source": [
        "bert_from_pickle = pd.read_pickle(r'/content/custom-T5')\n",
        "#bert_from_pickle= T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "for item in maintext:\n",
        "    summary=getSummarisedText(item)\n",
        "    print(summary)\n",
        "    predicted.append(summary)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> <extra_id_0> jobs 'false economy' - TUC Plans to shed 71,000 civil service jobs will prove to be a \"false economy\" that could hamper public sector reforms, according a TUC report. The TUC said cuts would deliver less than 6% of the <unk> £22bn ministers hope to save through efficiency reforms, according to a TUC report. Next Friday's strike action by the PCS is the biggest in the civil service since 1993, hitting Jobcentres, benefit agencies</s>\n",
            "<pad> <extra_id_0>, he is in no way anti-Jewish, I wouldn't for a moment work with him if he were. The probe follows the London mayor's comments to a Jewish journalist comparing him to a concentration camp guard, after a party about a week ago. The local government watchdog also said the allegation related to a failure to respect others. It has the power to suspend or bar Labour's Mr Livingstone from office. On Sunday, Deputy Mayor Nicky Gavron</s>\n",
            "<pad> <extra_id_0> plans Tony Blair has launched an attack on Conservative spending plans, saying they are a \"ludicrous improbability\". On Saturday, Tory shadow home secretary David Davis said the Tories would fund the cuts by removing \"inefficiencies\" which had \"burgeoned\" under Labour. In his speech, Mr Blair contrasted a reformed Labour party, which had learned to occupy the political centre ground, with a hidebound Tory party, which he said would turn the clock back with spending cuts</s>\n",
            "<pad> <extra_id_0>, that military action was lawful under the existing Security Council resolutions. In his new book, Lawless World, Philippe Sands, a QC and international law professor, suggests the parliamentary answer was written in Downing Street. The government has consistently refused to publish Lord Goldsmith's advice on the legality of the war - saying such papers have always been kept confidential. But the Conservatives and Liberal Democrats say they want the publication of the full legal advice given by the Attorney General. It said it was \"plain\" Iraq continued to be in material breach of UN resolution 1441. He said the claims suggested Parliament had only received a pr&#233;cis of Lord Goldsmith's\n",
            "<pad> <extra_id_0> MPs reveal sexist taunts Women MPs endure \"shocking\" levels of sexist abuse at the hands of their male counterparts, a new study shows. Male MPs pretended to juggle imaginary breasts and jeered \"melons\" as women made Commons speeches, researchers from Birkbeck College were told. The research team, under Professor Joni Lovenduski, had set out to look at the achievements and experiences of women at Westminster. But what emerged was complaints from MPs of</s>\n",
            "<pad> <extra_id_0> :summarize: Parties build up poll war chests The Labour Party received more than <unk> £5m in donations in the final quarter of 2004, new figures show. This is nearly half of the <unk> £11,724,929 received by 16 political parties listed by the Electoral Commission. The Conservatives were in second place with donations totalling <unk> £4,610,849, while the Liberal Democrats received just over <unk> £1m. The majority of Labour's donations came from affiliated trade unions. The</s>\n",
            "<pad> <extra_id_0> s Scotland's justice minister has warned bigoted soccer fans that she wants to hit them \"where it hurts most\" by banning them from matches. The sectarianism long associated with sections of the support from both clubs has become a significant target for the executive. She praised Celtic and Rangers for their work in tackling the problem. However, the minister said stopping sectarian abuse associated with Old Firm matches is a key objective.</s>\n",
            "<pad> <extra_id_0> :summarize: School sport 'is back', says PM Tony Blair has promised that \"sport is back\" as a priority for schools. The prime minister launched a <unk> £500m initiative to allow school sports clubs in England to provide up to three hours of extra activity a week by 2010. \"It's an important part of education and it's an important part of health,\" Mr Blair said. But the Conservatives say government proposals - which include two hours' PE within school for 75% of pupils by</s>\n",
            "<pad> <extra_id_0> :summarize: Hunt ban support is 'in decline' Support for a ban on hunting has fallen in the past six years, a poll suggests. Less than half the UK wants a ban compared to almost two-thirds in 1999, the Mori survey of 2,000 adults for BBC One's Countryfile programme suggests. The number opposed to a ban remains constant, but those \"neither supporting nor opposing\" has increased by 11%. Most city-dwellers support the ban but rural people</s>\n",
            "<pad> <extra_id_0>. Burglar defence guidelines issued Householders who injure or even kill intruders are unlikely to be prosecuted - providing they were acting \"honestly and instinctively\", new guidelines say. The law also protects those who use \"something to hand\" as a weapon, said the leaflet published jointly by the Crown Prosecution Service (CPS) and Association of Chief Police Officers (ACPO). In one of Britain's highest profile cases, Norfolk farmer Tony Martin was jailed for</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNd35qOrbQRN",
        "outputId": "2b458618-f1ac-4633-8613-03a04f745cd3"
      },
      "source": [
        "i=0\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "final_df = pd.DataFrame(columns=['Generated Text','Actual Text','Rogue Score'])\n",
        "for prediction,actual in zip(predicted,actuals):\n",
        "  try:\n",
        "   print(prediction)\n",
        "  #print(actual)\n",
        "   scores = scorer.score(actual,\n",
        "                      prediction)\n",
        "   final_df=final_df.append({'Generated Text':prediction,'Actual Text':actual,'Rogue Score':scores},ignore_index=True)\n",
        "   i+=1\n",
        "  except Exception as e:\n",
        "    print(\"ERROR\")\n",
        "final_df.to_csv('custompredictionspretrained.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> <extra_id_0> 'false economy' - TUC Plans to shed 71,000 civil service jobs will prove to be a \"false economy\" plans to shed 71,000 civil service jobs will prove to be a \"false economy\" that could hamper public sector reforms.</s>\n",
            "<pad> <extra_id_0> :summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize:summarize</s>\n",
            "<pad> Blair blasts conservative spending plans, saying they are a \"ludicrous improbability\" the prime minister has said the tories would cut <unk> £35bn in \"wasteful\" spending to allow <unk> £4bn in tax cuts.</s>\n",
            "<pad> <extra_id_0> a statement about Lord Goldsmith's position was published in his new book, Lawless World. the tories say ministers must respond in Parliament to claims that the legal advice used to justify the Iraq war was drawn up at Number 10. but the conservatives and liberal Democrats say they want the publication of the full legal advice given by the Attorney General.</s>\n",
            "<pad> <extra_id_0> jobs 'false economy' - TUC Plans to shed 71,000 civil service jobs will prove to be a \"false economy\" that could hamper public sector reforms, according a TUC report. The TUC said cuts would deliver less than 6% of the <unk> £22bn ministers hope to save through efficiency reforms, according to a TUC report. Next Friday's strike action by the PCS is the biggest in the civil service since 1993, hitting Jobcentres, benefit agencies</s>\n",
            "<pad> <extra_id_0>, he is in no way anti-Jewish, I wouldn't for a moment work with him if he were. The probe follows the London mayor's comments to a Jewish journalist comparing him to a concentration camp guard, after a party about a week ago. The local government watchdog also said the allegation related to a failure to respect others. It has the power to suspend or bar Labour's Mr Livingstone from office. On Sunday, Deputy Mayor Nicky Gavron</s>\n",
            "<pad> <extra_id_0> plans Tony Blair has launched an attack on Conservative spending plans, saying they are a \"ludicrous improbability\". On Saturday, Tory shadow home secretary David Davis said the Tories would fund the cuts by removing \"inefficiencies\" which had \"burgeoned\" under Labour. In his speech, Mr Blair contrasted a reformed Labour party, which had learned to occupy the political centre ground, with a hidebound Tory party, which he said would turn the clock back with spending cuts</s>\n",
            "<pad> <extra_id_0>, that military action was lawful under the existing Security Council resolutions. In his new book, Lawless World, Philippe Sands, a QC and international law professor, suggests the parliamentary answer was written in Downing Street. The government has consistently refused to publish Lord Goldsmith's advice on the legality of the war - saying such papers have always been kept confidential. But the Conservatives and Liberal Democrats say they want the publication of the full legal advice given by the Attorney General. It said it was \"plain\" Iraq continued to be in material breach of UN resolution 1441. He said the claims suggested Parliament had only received a pr&#233;cis of Lord Goldsmith's\n",
            "<pad> <extra_id_0> MPs reveal sexist taunts Women MPs endure \"shocking\" levels of sexist abuse at the hands of their male counterparts, a new study shows. Male MPs pretended to juggle imaginary breasts and jeered \"melons\" as women made Commons speeches, researchers from Birkbeck College were told. The research team, under Professor Joni Lovenduski, had set out to look at the achievements and experiences of women at Westminster. But what emerged was complaints from MPs of</s>\n",
            "<pad> <extra_id_0> :summarize: Parties build up poll war chests The Labour Party received more than <unk> £5m in donations in the final quarter of 2004, new figures show. This is nearly half of the <unk> £11,724,929 received by 16 political parties listed by the Electoral Commission. The Conservatives were in second place with donations totalling <unk> £4,610,849, while the Liberal Democrats received just over <unk> £1m. The majority of Labour's donations came from affiliated trade unions. The</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "6F4ho09EFvz5",
        "outputId": "c8672283-62ee-41a3-cda3-864fcb3c8aa6"
      },
      "source": [
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
        "    # Saving the dataframe as predictions.csv\n",
        "print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
        "for epoch in range(1):\n",
        "        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "        \n",
        "print('Output Files generated for review')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2111: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 0\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-e9e58ac13829>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactuals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         scores = scorer.score(actuals,\n\u001b[0;32m----> 8\u001b[0;31m                       predictions)\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mfinal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Generated Text'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Actual Text'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mactuals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Rogue Score'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mfinal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predictions.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rouge_score/rouge_scorer.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, target, prediction)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mtarget_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stemmer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mprediction_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stemmer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rouge_score/tokenize.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text, stemmer)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;31m# Convert everything to lowercase.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m   \u001b[0;31m# Replace any non-alpha-numeric characters with spaces.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"[^a-z0-9]+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGF4F2HKS_xt"
      },
      "source": [
        "i=0\n",
        "final_df = pd.DataFrame(columns=['Generated Text','Actual Text','Rogue Score'])\n",
        "for prediction in predictions:\n",
        "  scores = scorer.score(actuals[i],\n",
        "                      prediction)\n",
        "  final_df=final_df.append({'Generated Text':prediction,'Actual Text':actuals[i],'Rogue Score':scores},ignore_index=True)\n",
        "  i+=1\n",
        "final_df.to_csv('custompredictions.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0gIt4nimRig",
        "outputId": "88f94d89-1e16-42d1-efb2-24092670e0ae"
      },
      "source": [
        "!pip install pickle-mixin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pickle-mixin\n",
            "  Downloading https://files.pythonhosted.org/packages/02/77/9d5eb2201bbc130e2a5cc41fc949e4ab0da74b619107eac1c511be3af7a7/pickle-mixin-1.0.2.tar.gz\n",
            "Building wheels for collected packages: pickle-mixin\n",
            "  Building wheel for pickle-mixin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle-mixin: filename=pickle_mixin-1.0.2-cp37-none-any.whl size=6007 sha256=c1aec4d43993cdb0b4783add348e1d8c0585b427872f82cb98e2347c5e1194f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/05/42/71de70fa36b9cbb7657bb5793a16f8028c1cdc1bdd3b8e1ac3\n",
            "Successfully built pickle-mixin\n",
            "Installing collected packages: pickle-mixin\n",
            "Successfully installed pickle-mixin-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcfOE1g-morR",
        "outputId": "d9f77dc9-4ac9-4232-be00-eb15faa507ef"
      },
      "source": [
        "import pickle\n",
        "my_file = open(\"/content/custom-T5\", 'wb')\n",
        "my_file = pickle.dump(model,my_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "sDVBcOaOm174",
        "outputId": "5ef08a8d-4c29-4633-878d-75cfd0a773d3"
      },
      "source": [
        "#bert_from_pickle = pickle.load(my_file)\n",
        "bert_from_pickle = pd.read_pickle(r'/content/custom-T5')\n",
        "\n",
        "for epoch in range(1):\n",
        "        predictions, actuals = validate(epoch, tokenizer, bert_from_pickle, device, val_loader)\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2111: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-6aff72ffec9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactuals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_from_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-e5489420ad25>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(epoch, tokenizer, model, device, loader)\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mlength_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 )\n\u001b[1;32m     20\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerated_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m                 \u001b[0msynced_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m             )\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1792\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1793\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1794\u001b[0;31m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1795\u001b[0m             )\n\u001b[1;32m   1796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_output\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dim\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1633\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTbJwdLuouTz"
      },
      "source": [
        "def predict(epoch, tokenizer, model, device, loader):\n",
        "    model.\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data['target_ids'].to(device, dtype = torch.long)\n",
        "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask, \n",
        "                max_length=150, \n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5, \n",
        "                length_penalty=1.0, \n",
        "                early_stopping=True\n",
        "                )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "            if _%100==0:\n",
        "                print(f'Completed {_}')\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "    return predictions, actuals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pEsHnARpn1k",
        "outputId": "5d3c4373-a0cf-434f-bf92-3cb03eae9da6"
      },
      "source": [
        "# Concatenating the word \"summarize:\" to raw text\n",
        "text2 = \"summarize:\" + \"The all-party meeting called by Prime Minister Narendra Modi on June 24 is a major step towards the restoration of Jammu & Kashmir's statehood, a report said on June 20. Among those invited for the meeting include former J&K chief ministers Farooq Abdullah, Omar Abdullah, Mehbooba Mufti and Ghulam Nabi Azad.The CNN-News 18 report, which cited official sources, claimed that Modi would be discussing the roadmap for restoring J&K's statehood with the representatives of all political parties to be present at the meeting.Discussions, however, have been ruled out on restoring Article 370 - the now-abrogated law which granted J&K special status. The law was struck down on August 5, 2019, and the erstwhile state was bifurcated into the two union territories of Jammu & Kashmir and Ladakh.The status of Ladakh as a union territory will remain unchanged, the report said.\"\n",
        "#print(text2)\n",
        "# encoding the input text\n",
        "input_ids=tokenizer.encode(text2, return_tensors='pt')\n",
        "# Generating summary ids\n",
        "summary_ids = bert_from_pickle.generate(\n",
        "                input_ids = input_ids,\n",
        "                \n",
        "                max_length=150, \n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5, \n",
        "                length_penalty=1.0, \n",
        "                early_stopping=True\n",
        "                )\n",
        "# Decoding the tensor and printing the summary.\n",
        "t5_summary = tokenizer.decode(summary_ids[0])\n",
        "print(t5_summary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> the all-party meeting called by Prime Minister Narendra Modi on June 24 is a major step towards the restoration of Jammu & Kashmir's statehood, a report said on June 20. Among those invited for the meeting include former J&K chief ministers Farooq Abdullah, Omar Abdullah, Mehbooba Mufti and Ghulam Nabi Azad.</s>\n"
          ]
        }
      ]
    }
  ]
}