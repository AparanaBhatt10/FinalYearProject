{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERT-Keyword Extractor.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWA4_bY0eBPs"
      },
      "source": [
        "# Keyword-Extraction using BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45N8qn5-eBPu"
      },
      "source": [
        "Use BERT Token Classification Model to extract keyword tokens from a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkgXCBPdeBPu"
      },
      "source": [
        "## Prepare Dataset for BERT.\n",
        "\n",
        "Convert Sem-Eval 2010 keyword recognition dataset to BIO format dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ja8k4WuieBPv"
      },
      "source": [
        "import os\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIIhbHKzeBPv"
      },
      "source": [
        "train_path = \"KeyWordDataSet\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zej07V7eBPv"
      },
      "source": [
        "txt = sorted([f for f in os.listdir(train_path) if not f.endswith(\"-justTitle.txt\") and not f.endswith(\".key\") and not f.endswith(\"-CrowdCountskey\")])\n",
        "key = sorted([f for f in os.listdir(train_path) if f.endswith(\".key\")])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM7lXtq1eBPw",
        "outputId": "ffde2058-2cbe-4328-a2f3-0d9acb830b1a"
      },
      "source": [
        "filekey = dict()\n",
        "print(len(txt))\n",
        "print(len(key))\n",
        "for i, k in enumerate(txt):\n",
        "    try:\n",
        "       filekey[key[i]] = k\n",
        "      # print(train_path + \"/\" + filekey[key[i]])\n",
        "    except Exception as E:\n",
        "      print(E)\n",
        "\n",
        "    #print(k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "144\n",
            "144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TXj5MhNeBPw"
      },
      "source": [
        "def convert(key):\n",
        "    sentences = \"\"\n",
        "    \n",
        "    for line in open(train_path + \"/\" + filekey[key], 'r'):\n",
        "        sentences += (\" \" + line.rstrip())\n",
        "    tokens = sent_tokenize(sentences)\n",
        "    key_file = open(train_path + \"/\" + str(key),'r')\n",
        "    keys = [line.strip() for line in key_file]\n",
        "    key_sent = []\n",
        "    labels = []\n",
        "    for token in tokens:\n",
        "        z = ['O'] * len(token.split())\n",
        "        for k in keys:\n",
        "            if k in token:\n",
        "                \n",
        "                if len(k.split())==1:\n",
        "                    try:\n",
        "                        z[token.lower().split().index(k.lower().split()[0])] = 'B'\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "                elif len(k.split())>1:\n",
        "                    try:\n",
        "                        if token.lower().split().index(k.lower().split()[0]) and token.lower().split().index(k.lower().split()[-1]):\n",
        "                            z[token.lower().split().index(k.lower().split()[0])] = 'B'\n",
        "                            for j in range(1, len(k.split())):\n",
        "                                z[token.lower().split().index(k.lower().split()[j])] = 'I'\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "        for m, n in enumerate(z):\n",
        "            if z[m] == 'I' and z[m-1] == 'O':\n",
        "                z[m] = 'O'\n",
        "\n",
        "        if set(z) != {'O'}:\n",
        "            labels.append(z) \n",
        "            key_sent.append(token)\n",
        "    return key_sent, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cfXwtUIeBPx",
        "outputId": "c9512f7e-2936-4dd7-e528-975a9cd5aca2"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "sentences_ = []\n",
        "labels_ = []\n",
        "for key, value in filekey.items():\n",
        "  print(filekey[key])\n",
        "  if \".ipynb_checkpoints\" not in filekey[key]:\n",
        "    s, l = convert(key)\n",
        "    sentences_.append(s)\n",
        "    labels_.append(l)\n",
        "sentences = [item for sublist in sentences_ for item in sublist]\n",
        "labels = [item for sublist in labels_ for item in sublist]\n",
        "# print(len(sentences), len(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "C-41.txt\n",
            "C-42.txt\n",
            "C-44.txt\n",
            "C-45.txt\n",
            "C-46.txt\n",
            "C-48.txt\n",
            "C-49.txt\n",
            "C-50.txt\n",
            "C-52.txt\n",
            "C-53.txt\n",
            "C-54.txt\n",
            "C-55.txt\n",
            "C-56.txt\n",
            "C-57.txt\n",
            "C-58.txt\n",
            "C-61.txt\n",
            "C-62.txt\n",
            "C-65.txt\n",
            "C-66.txt\n",
            "C-67.txt\n",
            "C-68.txt\n",
            "C-69.txt\n",
            "C-71.txt\n",
            "C-72.txt\n",
            "C-74.txt\n",
            "C-75.txt\n",
            "C-76.txt\n",
            "C-77.txt\n",
            "C-78.txt\n",
            "C-79.txt\n",
            "C-80.txt\n",
            "C-81.txt\n",
            "C-83.txt\n",
            "C-84.txt\n",
            "H-35.txt\n",
            "H-37.txt\n",
            "H-38.txt\n",
            "H-40.txt\n",
            "H-41.txt\n",
            "H-42.txt\n",
            "H-43.txt\n",
            "H-44.txt\n",
            "H-45.txt\n",
            "H-46.txt\n",
            "H-47.txt\n",
            "H-48.txt\n",
            "H-49.txt\n",
            "H-50.txt\n",
            "H-52.txt\n",
            "H-53.txt\n",
            "H-54.txt\n",
            "H-60.txt\n",
            "H-61.txt\n",
            "H-62.txt\n",
            "H-63.txt\n",
            "H-64.txt\n",
            "H-69.txt\n",
            "H-73.txt\n",
            "H-77.txt\n",
            "H-79.txt\n",
            "H-81.txt\n",
            "H-82.txt\n",
            "H-83.txt\n",
            "H-84.txt\n",
            "H-85.txt\n",
            "H-87.txt\n",
            "H-88.txt\n",
            "H-90.txt\n",
            "H-92.txt\n",
            "H-95.txt\n",
            "H-96.txt\n",
            "H-97.txt\n",
            "H-98.txt\n",
            "I-37.txt\n",
            "I-38.txt\n",
            "I-42.txt\n",
            "I-43.txt\n",
            "I-45.txt\n",
            "I-46.txt\n",
            "I-47.txt\n",
            "I-48.txt\n",
            "I-49.txt\n",
            "I-50.txt\n",
            "I-51.txt\n",
            "I-52.txt\n",
            "I-53.txt\n",
            "I-54.txt\n",
            "I-55.txt\n",
            "I-56.txt\n",
            "I-57.txt\n",
            "I-58.txt\n",
            "I-59.txt\n",
            "I-60.txt\n",
            "I-61.txt\n",
            "I-62.txt\n",
            "I-63.txt\n",
            "I-64.txt\n",
            "I-65.txt\n",
            "I-66.txt\n",
            "I-68.txt\n",
            "I-70.txt\n",
            "I-71.txt\n",
            "I-72.txt\n",
            "I-73.txt\n",
            "I-74.txt\n",
            "I-75.txt\n",
            "I-76.txt\n",
            "I-77.txt\n",
            "J-33.txt\n",
            "J-34.txt\n",
            "J-35.txt\n",
            "J-36.txt\n",
            "J-37.txt\n",
            "J-38.txt\n",
            "J-39.txt\n",
            "J-40.txt\n",
            "J-41.txt\n",
            "J-42.txt\n",
            "J-44.txt\n",
            "J-45.txt\n",
            "J-47.txt\n",
            "J-49.txt\n",
            "J-50.txt\n",
            "J-51.txt\n",
            "J-52.txt\n",
            "J-53.txt\n",
            "J-55.txt\n",
            "J-56.txt\n",
            "J-57.txt\n",
            "J-58.txt\n",
            "J-59.txt\n",
            "J-60.txt\n",
            "J-61.txt\n",
            "J-62.txt\n",
            "J-63.txt\n",
            "J-65.txt\n",
            "J-66.txt\n",
            "J-67.txt\n",
            "J-69.txt\n",
            "J-70.txt\n",
            "J-71.txt\n",
            "J-72.txt\n",
            "J-73.txt\n",
            "J-74.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TS2RoiikfPEM",
        "outputId": "33de1cdd-7c2e-4054-c47b-68f36d5872d2"
      },
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 29.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 30 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 123 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.19.5)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.18.27.tar.gz (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 31.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.62.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.9.0+cu102)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n",
            "Collecting botocore<1.22.0,>=1.21.27\n",
            "  Downloading botocore-1.21.27-py3-none-any.whl (7.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.8 MB 42.3 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 52.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.27->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.27->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 49.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: boto3\n",
            "  Building wheel for boto3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for boto3: filename=boto3-1.18.27-py3-none-any.whl size=129020 sha256=be126aecf22a0f55b0571cba3c96bbd8a724398ac2490be5fc3b85fe0a7d5ebd\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/8c/12/2458a3e4630a0c93877365dfcc6449f0b02ccafa84a2baf5f3\n",
            "Successfully built boto3\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.18.27 botocore-1.21.27 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.5.0 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tSSzXYMeBPx"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OqsLTtceBPx"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd-vDAQveBPy"
      },
      "source": [
        "MAX_LEN = 75\n",
        "bs = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4--soWEeBPy"
      },
      "source": [
        "tag2idx = {'B': 0, 'I': 1, 'O': 2}\n",
        "tags_vals = ['B', 'I', 'O']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woaMxCc7eBPz"
      },
      "source": [
        "device = \"cpu\"\n",
        "n_gpu = torch.cuda.device_count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IM9qCAbeBPz",
        "outputId": "86692674-a41a-48fe-ce7a-a0e08dd9a119"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 313980.33B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvjZ34ereBPz"
      },
      "source": [
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTbD7Hu_eBPz",
        "outputId": "13d537c4-dff4-4212-ad29-06b3a072991e"
      },
      "source": [
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (766 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (714 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1001 > 512). Running this sequence through BERT will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efGjOG5teBPz"
      },
      "source": [
        "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dnz8aareBP0"
      },
      "source": [
        "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3AcWfl6eBP0"
      },
      "source": [
        "tr_inputs = torch.tensor(tr_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "tr_tags = torch.tensor(tr_tags)\n",
        "val_tags = torch.tensor(val_tags)\n",
        "tr_masks = torch.tensor(tr_masks)\n",
        "val_masks = torch.tensor(val_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTgwuZtIeBP0"
      },
      "source": [
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyulkd5CeBP0",
        "outputId": "77802d49-f0c8-4e4e-91a4-1e3f4561987c"
      },
      "source": [
        "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:33<00:00, 12042913.68B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaJAzurKW87T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYMVy9sTeBP0"
      },
      "source": [
        "model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya3j5cVfeBP1"
      },
      "source": [
        "FULL_FINETUNING = True\n",
        "if FULL_FINETUNING:\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.1},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.1}\n",
        "    ]\n",
        "else:\n",
        "    param_optimizer = list(model.classifier.named_parameters()) \n",
        "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y80gdW55hS0J",
        "outputId": "e1739562-3288-4698-f780-fe584f21f981"
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▌                        | 10 kB 29.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 43 kB 956 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=d084c014217387a527ecd17cccd1aa713c2f842c7b4a3e22250ed46f3bd87761\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPVO8U3BeBP1"
      },
      "source": [
        "from seqeval.metrics import f1_score\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVaQxVk7eBP1",
        "outputId": "91421344-602c-496a-ee58-327b40d85230"
      },
      "source": [
        "epochs = 4\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    # TRAIN loop\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # forward pass\n",
        "        loss = model(b_input_ids, token_type_ids=None,\n",
        "                     attention_mask=b_input_mask, labels=b_labels)\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        # track train loss\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        model.zero_grad()\n",
        "    # print train loss per epoch\n",
        "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    # VALIDATION on validation set\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions , true_labels = [], []\n",
        "    for batch in valid_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
        "                                  attention_mask=b_input_mask, labels=b_labels)\n",
        "            logits = model(b_input_ids, token_type_ids=None,\n",
        "                           attention_mask=b_input_mask)\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "        true_labels.append(label_ids)\n",
        "        \n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        eval_loss += tmp_eval_loss.mean().item()\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        \n",
        "        nb_eval_examples += b_input_ids.size(0)\n",
        "        nb_eval_steps += 1\n",
        "    eval_loss = eval_loss/nb_eval_steps\n",
        "    print(\"Validation loss: {}\".format(eval_loss))\n",
        "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
        "    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
        "    #print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtsTkve0dvXv"
      },
      "source": [
        "import pickle\n",
        "my_file = open(\"/content/key-bert\", 'wb')\n",
        "my_file = pickle.dump(model,my_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxKuAqFddwzW"
      },
      "source": [
        "model=pd.read_pickle(r'/content/key-bert')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6tw2hBgeBP2"
      },
      "source": [
        "."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l0rRNxleBP2",
        "outputId": "f20a6e53-79b7-4561-a9fb-8e3dad3f04ff"
      },
      "source": [
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "for batch in valid_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
        "                              attention_mask=b_input_mask, labels=b_labels)\n",
        "        logits = model(b_input_ids, token_type_ids=None,\n",
        "                       attention_mask=b_input_mask)\n",
        "        \n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    true_labels.append(label_ids)\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "    eval_loss += tmp_eval_loss.mean().item()\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    nb_eval_examples += b_input_ids.size(0)\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\n",
        "valid_tags = [[tags_vals[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\n",
        "print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
        "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "#print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.2017330676317215\n",
            "Validation Accuracy: 0.9566880952380953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c1OojB-eBP3"
      },
      "source": [
        "### Get keywords from sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVryGSgOeBP3"
      },
      "source": [
        "def keywordextract(sentence):\n",
        "    text = sentence\n",
        "    tkns = tokenizer.tokenize(text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tkns)\n",
        "    segments_ids = [0] * len(tkns)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
        "    segments_tensors = torch.tensor([segments_ids]).to(device)\n",
        "    model.eval()\n",
        "    prediction = []\n",
        "    logit = model(tokens_tensor, token_type_ids=None,\n",
        "                                  attention_mask=segments_tensors)\n",
        "    #logit = logit.detach().cpu().numpy()\n",
        "    prediction.extend([list(p) for p in np.argmax(logit, axis=2)])\n",
        "    for k, j in enumerate(prediction[0]):\n",
        "        if j==1 or j==0:\n",
        "            print(tokenizer.convert_ids_to_tokens(tokens_tensor[0].to('cpu').numpy())[k], j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdKjsrH4eBP3"
      },
      "source": [
        "#text = \"Business Wire IndiaAdding another feather to its cap, Angel Broking which enable clients to avail services digitally. Our customer outreach spans across approximately 97.6% or 18,797 pin codes in India. ABL manages\"\n",
        "\n",
        "text=\"Two men have been arrested on charges of cheating after a shooter was lured into selling his phone and air pistol for an exorbitant amount of cash but was given a bag containing banknote-size papers instead, the police said on Wednesday.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "RcuQFF57eBP3",
        "outputId": "4081347a-29b8-4afc-da8e-f26225a8c013"
      },
      "source": [
        "keywordextract(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-7bdea9ebab08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeywordextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-3e2a6e034cb6>\u001b[0m in \u001b[0;36mkeywordextract\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mindexed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtkns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msegments_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtkns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtokens_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexed_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msegments_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegments_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "gpA1dra3YnmZ",
        "outputId": "65277fc4-6785-4962-ac25-a7340c648dc2"
      },
      "source": [
        "keywordextract(text2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-514760e45ba8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeywordextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'text2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dbzy0aX3eBP4"
      },
      "source": [
        "model.s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kdodzEZVHLg"
      },
      "source": [
        "import pickle\n",
        "my_file = open(\"/content/key-bert\", 'wb')\n",
        "my_file = pickle.dump(model,my_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}